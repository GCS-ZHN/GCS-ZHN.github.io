<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="GNN学习笔记"><meta name="keywords" content="gnn,深度学习,图神经网络,AI,人工智能"><meta name="author" content="GCS-ZHN,undefined"><meta name="copyright" content="GCS-ZHN"><title>GNN学习笔记【潇洒记忆】</title><link rel="stylesheet" href="/css/fan.css"><link rel="stylesheet" href="/css/thirdparty/jquery.mCustomScrollbar.min.css"><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"><link rel="icon" href="/favicon.ico"><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- script(src=url_for("/js/mathjax/mathjax.js"))--><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
});
</script><script>var isPassword = '' || false;
if (isPassword) {
    if (prompt('请输入文章密码') !== '') {
        alert('密码错误！');
        history.back();
    }
}</script><script>window.GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  gitment: {},
  valine: {},
}</script><meta name="generator" content="Hexo 6.0.0"></head><body><canvas id="universe"></canvas><!--#body--><div id="sidebar"><div class="toggle-sidebar-info button-hover"><span data-toggle="文章目录">站点概览</span></div><div class="sidebar-toc"><div class="sidebar-toc-title">目录</div><div class="sidebar-toc-progress"><span class="progress-notice">您已阅读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc-progress-bar"></div></div><div class="sidebar-toc-content" id="sidebar-toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E5%9B%BE%E7%9A%84%E6%A6%82%E8%BF%B0"><span class="toc-number">1.</span> <span class="toc-text">一、图的概述</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E5%AF%BC%E6%95%B0%E3%80%81%E6%A2%AF%E5%BA%A6%E4%B8%8E%E6%95%A3%E5%BA%A6"><span class="toc-number">2.</span> <span class="toc-text">二、导数、梯度与散度</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-%E5%AF%BC%E6%95%B0"><span class="toc-number">2.1.</span> <span class="toc-text">2.1 导数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-%E6%A2%AF%E5%BA%A6-gradient"><span class="toc-number">2.2.</span> <span class="toc-text">2.2 梯度 (gradient)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-%E6%95%A3%E5%BA%A6-%EF%BC%88divergence%EF%BC%89"><span class="toc-number">2.3.</span> <span class="toc-text">2.3 散度 （divergence）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="toc-number">2.4.</span> <span class="toc-text">参考文献</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E7%9F%A9%E9%98%B5%E3%80%81%E7%89%B9%E5%BE%81%E5%80%BC%E4%B8%8E%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F"><span class="toc-number">3.</span> <span class="toc-text">三、矩阵、特征值与特征向量</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97"><span class="toc-number">3.1.</span> <span class="toc-text">3.1 矩阵运算</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%93%88%E8%BE%BE%E7%8E%9B%E7%A7%AF"><span class="toc-number">3.1.1.</span> <span class="toc-text">哈达玛积</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-%E6%AD%A3%E4%BA%A4%E7%9F%A9%E9%98%B5"><span class="toc-number">3.2.</span> <span class="toc-text">3.2 正交矩阵</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3-%E7%89%B9%E5%BE%81%E5%80%BC%E3%80%81%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F%E5%8F%8A%E7%89%B9%E5%BE%81%E6%96%B9%E7%A8%8B"><span class="toc-number">3.3.</span> <span class="toc-text">3.3 特征值、特征向量及特征方程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-4-%E7%9B%B8%E4%BC%BC%E7%9F%A9%E9%98%B5"><span class="toc-number">3.4.</span> <span class="toc-text">3.4 相似矩阵</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-5-%E5%AF%B9%E7%A7%B0%E7%9F%A9%E9%98%B5%E7%9A%84%E5%AF%B9%E8%A7%92%E5%8C%96"><span class="toc-number">3.5.</span> <span class="toc-text">3.5 对称矩阵的对角化</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E5%9B%BE%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2"><span class="toc-number">4.</span> <span class="toc-text">四、图傅里叶变换</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1-%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E7%AE%97%E5%AD%90"><span class="toc-number">4.1.</span> <span class="toc-text">4.1 拉普拉斯算子</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2-%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E7%9F%A9%E9%98%B5"><span class="toc-number">4.2.</span> <span class="toc-text">4.2 拉普拉斯矩阵</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-3-%E5%9B%BE%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2%E7%9A%84%E5%AE%9A%E4%B9%89"><span class="toc-number">4.3.</span> <span class="toc-text">4.3 图傅里叶变换的定义</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-4-%E7%A9%BA%E5%9F%9F%E5%88%B0%E9%A2%91%E5%9F%9F%E7%9A%84%E8%BD%AC%E6%8D%A2"><span class="toc-number">4.4.</span> <span class="toc-text">4.4 空域到频域的转换</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE-1"><span class="toc-number">4.5.</span> <span class="toc-text">参考文献</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E5%9B%BE%E6%BB%A4%E6%B3%A2%E5%99%A8"><span class="toc-number">5.</span> <span class="toc-text">五、图滤波器</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#5-1-%E7%A9%BA%E5%9F%9F%E8%A7%92%E5%BA%A6"><span class="toc-number">5.1.</span> <span class="toc-text">5.1 空域角度</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-2-%E9%A2%91%E5%9F%9F%E8%A7%92%E5%BA%A6"><span class="toc-number">5.2.</span> <span class="toc-text">5.2 频域角度</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%AD%E3%80%81%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">6.</span> <span class="toc-text">六、图卷积神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#6-1-%E5%9B%BE%E5%8D%B7%E7%A7%AF"><span class="toc-number">6.1.</span> <span class="toc-text">6.1 图卷积</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E5%AE%9A%E7%90%86"><span class="toc-number">6.1.1.</span> <span class="toc-text">卷积定理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%9A%84%E5%9F%BA%E7%A1%80%E5%AE%9A%E4%B9%89"><span class="toc-number">6.1.2.</span> <span class="toc-text">图卷积的基础定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%9A%84%E9%80%9A%E9%81%93"><span class="toc-number">6.1.3.</span> <span class="toc-text">图卷积的通道</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-2-%E4%BB%A5%E9%A2%91%E7%8E%87%E5%93%8D%E5%BA%94%E7%9F%A9%E9%98%B5%E4%B8%BA%E5%AD%A6%E4%B9%A0%E5%8F%82%E6%95%B0"><span class="toc-number">6.2.</span> <span class="toc-text">6.2 以频率响应矩阵为学习参数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-3-%E4%BB%A5%E5%A4%9A%E9%A1%B9%E5%BC%8F%E7%B3%BB%E6%95%B0%E4%B8%BA%E5%AD%A6%E4%B9%A0%E5%8F%82%E6%95%B0"><span class="toc-number">6.3.</span> <span class="toc-text">6.3 以多项式系数为学习参数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-4-%E5%9B%BA%E5%AE%9A%E5%9B%BE%E6%BB%A4%E6%B3%A2%E5%99%A8%E2%80%94%E2%80%94GCN"><span class="toc-number">6.4.</span> <span class="toc-text">6.4 固定图滤波器——GCN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%83%E3%80%81%E5%9F%BA%E4%BA%8E%E7%A9%BA%E5%9F%9F%E7%9A%84%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">6.5.</span> <span class="toc-text">七、基于空域的图神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#7-1-%E8%B7%9D%E7%A6%BB%E7%A9%BA%E9%97%B4"><span class="toc-number">6.5.1.</span> <span class="toc-text">7.1 距离空间</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-2-%E5%8E%8B%E7%BC%A9%E6%98%A0%E5%B0%84"><span class="toc-number">6.5.2.</span> <span class="toc-text">7.2 压缩映射</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-3-%E5%B7%B4%E6%8B%BF%E8%B5%AB%E4%B8%8D%E5%8A%A8%E7%82%B9%E5%AE%9A%E7%90%86"><span class="toc-number">6.5.3.</span> <span class="toc-text">7.3 巴拿赫不动点定理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE-2"><span class="toc-number">6.6.</span> <span class="toc-text">参考文献</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info-avatar"><img class="author-info-avatar-img" src="/avatar.png"></div><div class="author-info-name">GCS-ZHN</div><div class="author-info-description"></div><div class="links-buttons"><a class="links-button button-hover" href="https://github.com/GCS-ZHN" target="_blank">GitHub<i class="icon-dot bg-color3"></i></a><a class="links-button button-hover" href="mailto:zhang.h.n@foxmail.com" target="_blank">E-Mail<i class="icon-dot bg-color2"></i></a></div><div class="author-info-articles"><a class="author-info-articles-archives article-meta" href="/archives"><span class="pull-top">日志</span><span class="pull-bottom">22</span></a><a class="author-info-articles-tags article-meta" href="/tags"><span class="pull-top">标签</span><span class="pull-bottom">47</span></a><a class="author-info-articles-categories article-meta" href="/categories"><span class="pull-top">分类</span><span class="pull-bottom">15</span></a></div></div></div><div id="main-container"><header><div id="menu-outer"><i class="menu-list-icon fas fa-bars"></i><nav id="menu-inner"><a class="menu-item" href="/">首页</a><a class="menu-item" href="/archives">归档</a></nav><div class="right-info"><a class="title-name" href="/">潇洒记忆</a><span id="now-time"></span></div></div></header><div id="content-outer"><div id="content-inner"><article id="post"><div class="post-header"><div class="title">GNN学习笔记</div><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 发表于 2022-03-10 | 更新于 2022-08-01</time><!--time.button-hover.post-date #[i.fas.fa-calendar-alt.article-icon(aria-hidden="true")] #[=__('post.modified')] #[=date(page['updated'], config.date_format)]--><div class="button-hover categories"><i class="fa fa-inbox article-icon" aria-hidden="true"></i><a class="link-a" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a></div><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="/tags/gnn/">gnn</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="/tags/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">图神经网络</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="/tags/AI/">AI</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a></div></div></div><div class="main-content"><h1 id="一、图的概述"><a href="#一、图的概述" class="headerlink" title="一、图的概述"></a>一、图的概述</h1><p>在图论中，图（Graph）指的许多节点及其相互关系。而我们传统的图像可以看着是一种特殊的图——像素点及其之间的关系。如果有点集$V={V_1,V_2,…,V_n}$<br>和他们之间的关系边集$E={e_1,e_2,…,e_n}$。那么我们一般用下列数学表达式来表示一个图。</p>
<script type="math/tex; mode=display">G=(V,E)\tag{1-1}</script><ol>
<li><p>有向图VS无向图<br>有向图的边具有方向性，$e_{ij}$表示从$v_i$指向$v_j$的有向边。但注意有向图不一定所有边都是有向边。<br>无向图的边没有方向性。</p>
</li>
<li><p>加权图VS非加权图</p>
<p> 加权指的是两点之间的边有权重。而非加权图则相当于时权重相等的加权图</p>
</li>
<li><p>连通图VS非连通图</p>
<p> 若<strong>任意两个节点都有通路，称为连通图</strong>。反之则称为非连通图。非连通图中有许多连通分量。连通分量自身是图的最大连通子图。连通图的连通分量是自己。</p>
</li>
<li><p>子图</p>
<p>子图从直观来看是图的裁剪区域。从数学表达来看，子图的边集和点集均为图的边集和点集的子集，即</p>
<script type="math/tex; mode=display">
 G'=(V',E') \\
 V'\subseteq V \\
 E'\subseteq E \tag{1-2}</script></li>
<li><p>通路与距离</p>
<p> 通路也称路径，对于两个点，他们的通路就是连通它们的边的序列集合。</p>
<script type="math/tex; mode=display">P_{ij}=(e_1, e_2, ..., e_n)\tag{1-3}</script><p> <strong>两点间的距离指的是两点间最短通路的边数</strong>。</p>
<script type="math/tex; mode=display">d(v_i,v_j)=\min(|P_{ij}|)\tag{1-4}</script><p> k阶邻居：指的是两点的距离为k，此时两点互为k阶邻居（无向图）。</p>
<p> k阶子图：指的是任意两点之间距离最大为k的子图。</p>
</li>
<li><p>邻接矩阵VS关联矩阵</p>
<p> 邻接矩阵表达的是点与点之间存在连接，即边。关联矩阵表达的是点和边之间存在关联，即点是边的端点。两种矩阵都能够表达图中的关系。</p>
</li>
<li><p>同构图VS异构图</p>
<p>同构图中所有节点的类型都是相同的，边的类型也是相同的。而异构图则不一定。异构图更有现实意义。</p>
<span id="more"></span>
<h1 id="二、导数、梯度与散度"><a href="#二、导数、梯度与散度" class="headerlink" title="二、导数、梯度与散度"></a>二、导数、梯度与散度</h1><h2 id="2-1-导数"><a href="#2-1-导数" class="headerlink" title="2.1 导数"></a>2.1 导数</h2><p>导数反映的$f:X\rightarrow Y$这样一个映射中，$Y$关于$X$的变化速率问题，<strong>是一个标量（scalar）概念</strong>。对于一维映射，其形式非常简单，即微商：</p>
<script type="math/tex; mode=display">f'(x)=\frac{df}{dx}\tag{2-1}</script><p>而对于多维映射，则有偏导数和偏导数。偏导数是相对其中一个维度的导数。</p>
<script type="math/tex; mode=display">f_x'=\frac{\partial f}{\partial x}\tag{2-2}</script><p>全导数的本质是降维。它将多维统一到一维。</p>
<script type="math/tex; mode=display">\frac{dz}{dt}=\frac{\partial z}{\partial w}. \frac{dw}{dt} + \frac{\partial z}{\partial v}. \frac{dv}{dt}\tag{2-3}</script></li>
</ol>
<h2 id="2-2-梯度-gradient"><a href="#2-2-梯度-gradient" class="headerlink" title="2.2 梯度 (gradient)"></a>2.2 梯度 (gradient)</h2><p>其最直接的物理意义代表是速度，<strong>是一个矢量（vector）概念</strong>。对于一维映射，其值就等于其导数，方向为切线方向。而对于多维空间，则为各个维度梯度分量的矢量和，体现上为偏导数向量组。</p>
<script type="math/tex; mode=display">
\triangledown\boldsymbol{f}=(
    \frac{\partial f}{\partial x_1},
    \frac{\partial f}{\partial x_2},
    ...,
    \frac{\partial f}{\partial x_n}
)\tag{2-4}</script><h2 id="2-3-散度-（divergence）"><a href="#2-3-散度-（divergence）" class="headerlink" title="2.3 散度 （divergence）"></a>2.3 散度 （divergence）</h2><p>物理意义在于发散源或吸收源的判断，<strong>是一个标量概念</strong>。对于一个梯度$\triangledown\boldsymbol{f}$的单位区域，散度表示的是净流量的大小。对于一个微小截面$\delta S$和微小体积$\delta V$，设其截面法向量$\boldsymbol{n}$，则有散度定义（单位体积变化率）</p>
<script type="math/tex; mode=display">\text{div }f=\lim_{\delta V\rightarrow 0}\frac{1}{\delta V}\int_{\delta S}\triangledown\boldsymbol{f}\cdot\boldsymbol{n}dS\tag{2-5}</script><p>代入梯度公式，在n维空间内可转为</p>
<script type="math/tex; mode=display">\text{div }f=\sum_{i=1}^n\frac{\partial f}{\partial x_i}\tag{2-6}</script><p>对于一个向量场，一个微小区域如果散度大于0，那么显然该处是有发散源，如果等于0，那么显然无源。而小于0，代表是吸收源。例如，一个磁场区域如果没有源，那么磁感线几条进入几条出去，磁通量肯定为0。如果出去多进来少（净出大于0，净速率大于0），那么也就是发散；反之，则为吸收。<br>
<img src="/2022/03/10/GNN/div.jpg" class="" title="divrgence">
</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/24074028">如何直观形象的理解梯度</a></p>
<h1 id="三、矩阵、特征值与特征向量"><a href="#三、矩阵、特征值与特征向量" class="headerlink" title="三、矩阵、特征值与特征向量"></a>三、矩阵、特征值与特征向量</h1><h2 id="3-1-矩阵运算"><a href="#3-1-矩阵运算" class="headerlink" title="3.1 矩阵运算"></a>3.1 矩阵运算</h2><h3 id="哈达玛积"><a href="#哈达玛积" class="headerlink" title="哈达玛积"></a>哈达玛积</h3><p>对应位置元素相乘。对矩阵$A<em>{m \times n}和B</em>{m \times n}$，有</p>
<script type="math/tex; mode=display">
A \odot B = \begin{bmatrix}
    a_{11}b_{11} & a_{12}b_{12} & \cdots & a_{1n}b_{1n} \\
    a_{21}b_{11} & a_{22}b_{22} & \cdots & a_{2n}b_{2n} \\
    \vdots       & \vdots       & \ddots & \vdots       \\
    a_{m1}b_{m1} & a_{m2}b_{m2} & \cdots & a_{mn}b_{mn}
\end{bmatrix}</script><h2 id="3-2-正交矩阵"><a href="#3-2-正交矩阵" class="headerlink" title="3.2 正交矩阵"></a>3.2 正交矩阵</h2><p>正交矩阵（必须是方阵）的列向量组是一组标准正交基。标准正交基其实就是n维向量空间内两两正交的一组单位向量。即$A=(e_1, e_2, …, e_n)$。正交矩阵具有以下特性：</p>
<script type="math/tex; mode=display">A^TA=E 即 A^T=A^{-1}\tag{3-1}</script><h2 id="3-3-特征值、特征向量及特征方程"><a href="#3-3-特征值、特征向量及特征方程" class="headerlink" title="3.3 特征值、特征向量及特征方程"></a>3.3 特征值、特征向量及特征方程</h2><p>我们知道在差分方程和微分方程中有特征值的概念，它是方程组的稳定性分析的基础概念。在矩阵中，同样具有特征值。</p>
<p>对n阶方阵A，存在数$\lambda$和n维非零列向量$x$，满足</p>
<script type="math/tex; mode=display">
Ax=\lambda x     \tag{3-2}</script><p>则称$\lambda$为特征值，而$x$为对象的特征向量。(3-2)式可以转化为</p>
<script type="math/tex; mode=display">
(A-\lambda E)x=0 \tag{3-3}</script><p>而对于上述方程，有</p>
<script type="math/tex; mode=display">
\because |\bold{0}|=|\boldsymbol{(A-\lambda E)x}|=|A-\lambda E||x| \And |x|\not = 0    \\
\therefore |A-\lambda E|=0\tag{3-4}</script><p>方程(3-4)称为矩阵A的特征方程。$|A-\lambda E|$称为矩阵A的特征多项式。当$\lambda \in C$（复数集）时，对n阶方阵A，必然存在n个特征值$(\lambda_1,\lambda_2,…,\lambda_n)$（重根得重算）。且有</p>
<script type="math/tex; mode=display">\sum_i  \lambda_i = \sum_i a_{ii} \tag{3-5}</script><script type="math/tex; mode=display">\prod_i \lambda_i = |A|           \tag{3-6}</script><p>注意零矩阵的模为0，但为模为0不一定是零矩阵。因此，代入特征根，可以求得对应的非零向量$x$，即特征向量。</p>
<h2 id="3-4-相似矩阵"><a href="#3-4-相似矩阵" class="headerlink" title="3.4 相似矩阵"></a>3.4 相似矩阵</h2><p>设$A、B$都是$n$阶方阵，若存在可逆矩阵$P$，有</p>
<script type="math/tex; mode=display">\boldsymbol{P^{-1}AP=B}  \tag{3-7}</script><p>则称矩阵A与矩阵B相似，可逆矩阵P被称为<strong>相似变换矩阵</strong>。对于相似矩阵，容易证明具有相同的特征方程和特征根。</p>
<script type="math/tex; mode=display">
|B-\lambda E|=|P^{-1}AP-\lambda E|         \\
             =|P^{-1}AP-P^{-1}\lambda EP|  \\
             =|P^{-1}(A-\lambda E)P|       \\
             =|P^{-1}||A-\lambda E||P|     \\
             =|A-\lambda E|        \tag{3-8}</script><p>当相似矩阵为对角矩阵$\boldsymbol{\Lambda}$</p>
<script type="math/tex; mode=display">
\boldsymbol{\Lambda}=
\begin{bmatrix}
    \lambda_1               \\
    & \lambda_2             \\
    && \ddots               \\
    &&& \lambda_n
\end{bmatrix}
\tag{3-9}</script><p>则主对角线的值即为特征值。设对应的相似变换矩阵列向量组为$P=(p_1,p_2,\cdots,p_n)$。可以证明该列向量组为对应的特征向量。</p>
<script type="math/tex; mode=display">
\because P^{-1}AP=\boldsymbol{\Lambda} \\
\therefore AP=P\boldsymbol{\Lambda}    \\
\because AP=(Ap_1, Ap_2, \cdots,Ap_n)  \\
\because P\boldsymbol{\Lambda}=(\lambda_1p_1, \lambda_2p_2, \cdots,\lambda_np_n) \\
\therefore Ap_i=\lambda_i p_i</script><p>而这样一个变换，称为<strong>对角化</strong>。其核心问题是能否对角化，相似变换矩阵P怎么求。</p>
<h2 id="3-5-对称矩阵的对角化"><a href="#3-5-对称矩阵的对角化" class="headerlink" title="3.5 对称矩阵的对角化"></a>3.5 对称矩阵的对角化</h2><p>对于一个方阵$A$，若满足$A=A^T$，即称为对称矩阵。容易证明，对称矩阵的特征向量必然两两正交。且有定理说明一个实对称矩阵必能被<strong>正交对角化</strong>，即必然存在正交矩阵$P$，有</p>
<script type="math/tex; mode=display">P^{-1}AP=P^TAP=\Lambda \tag{3-10}</script><p>其中$\Lambda$为对角矩阵。</p>
<h1 id="四、图傅里叶变换"><a href="#四、图傅里叶变换" class="headerlink" title="四、图傅里叶变换"></a>四、图傅里叶变换</h1><h2 id="4-1-拉普拉斯算子"><a href="#4-1-拉普拉斯算子" class="headerlink" title="4.1 拉普拉斯算子"></a>4.1 拉普拉斯算子</h2><script type="math/tex; mode=display">\triangle f=\sum_{i=1}^n\frac{\partial^2f}{\partial x_i^2} \tag{4-1}</script><p>拉普拉斯算子用于表示n维欧式空间中f的梯度$\bigtriangledown f$的散度，是二阶微分算子。对于二维空间，简化为</p>
<script type="math/tex; mode=display">\triangle f(x,y)=\frac{\partial^2f}{\partial x^2}+\frac{\partial^2f}{\partial y^2} \tag{4-2}</script><p>而从差分近似的角度，有</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial^2f}{\partial x^2}=f(x+1, y)-2f(x,y)+f(x-1, y) \\
\frac{\partial^2f}{\partial y^2}=f(x, y+1)-2f(x,y)+f(x, y+1)
\end{aligned}
\tag{4-3}</script><p>从而有</p>
<script type="math/tex; mode=display">\triangle f(x,y)=f(x-1,y)+f(x+1,y)+f(x,y+1)+f(x,y-1)-4f(x,y) \tag{4-4}</script><p>转化为卷积核，有</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
    0 &  1  & 0 \\
    1 & -4  & 1 \\
    0 &  1  & 0
\end{bmatrix}
\tag{4-5}</script><p>可以看到元素总和为0。利用拉普拉斯算子做图像处理，可以得到图像的散度矩阵，它描绘了像素与周围的差异大小，因此该算子常用于边缘检测（和之前的Sobel算子类似）。</p>
<h2 id="4-2-拉普拉斯矩阵"><a href="#4-2-拉普拉斯矩阵" class="headerlink" title="4.2 拉普拉斯矩阵"></a>4.2 拉普拉斯矩阵</h2><p>对于一个图$G=(V,E)$<br>
<img src="/2022/03/10/GNN/graph.png" class="" title="graph">
</p>
<p>$A$为其顶点之间的邻接矩阵。</p>
<script type="math/tex; mode=display">
A=
\begin{bmatrix}
    0 & 1 & 0 & 0 & 1 & 0 \\
    1 & 0 & 1 & 0 & 1 & 0 \\
    0 & 1 & 0 & 1 & 0 & 0 \\
    0 & 0 & 1 & 0 & 1 & 1 \\
    1 & 1 & 0 & 1 & 0 & 0 \\
    0 & 0 & 0 & 1 & 0 & 0
\end{bmatrix}
\tag{4-6}</script><p>一个顶点的度$\text{deg}(v_i)$指的是一个顶点$v_i$的连接总数。对于有向图，度可以分为出度和入度。以顶点的度构成的对角矩阵称为度矩阵$D$。</p>
<script type="math/tex; mode=display">
D=
\begin{bmatrix}
    2 & 0 & 0 & 0 & 0 & 0 \\
    0 & 3 & 0 & 0 & 0 & 0 \\
    0 & 0 & 2 & 0 & 0 & 0 \\
    0 & 0 & 0 & 3 & 0 & 0 \\
    0 & 0 & 0 & 0 & 3 & 0 \\
    0 & 0 & 0 & 0 & 0 & 1
\end{bmatrix}
\tag{4-7}</script><p>而拉普拉斯矩阵的定义为</p>
<script type="math/tex; mode=display">
L=D-A \\
L=
\begin{bmatrix}
     2 & -1 &  0 &  0 & -1 &  0 \\
    -1 &  3 & -1 &  0 & -1 &  0 \\
     0 & -1 &  2 & -1 &  0 &  0 \\
     0 &  0 & -1 &  3 & -1 & -1 \\
    -1 & -1 &  0 & -1 &  3 &  0 \\
     0 &  0 &  0 & -1 &  0 &  1
\end{bmatrix}
\tag{4-8}</script><p>用数学公式直接表达，其元素级别为</p>
<script type="math/tex; mode=display">
L_{ij}=
\begin{cases}
    \text{deg}(v_i) & \text{if } i=j \\
    -1              & \text{if } e_{ij}\in E \\
    0               & \text{otherwise} 
\end{cases}
\tag{4-9}</script><p>用$\sqrt{\text{deg}(v_i)\text{deg}(v_j)}$进行归一化，其元素级别为</p>
<script type="math/tex; mode=display">
L_{ij}=
\begin{cases}
    1               & \text{if } i=j \\
    -\frac{1}{\sqrt{\text{deg}(v_i)\text{deg}(v_j)}} & \text{if } e_{ij}\in E \\
    0               & \text{otherwise} 
\end{cases}
\tag{4-10}</script><p><strong>可以看到不论是按行还是按列压缩求和，和都为0</strong>。</p>
<p>拉普拉斯矩阵是拉普拉斯算子在图上的离散推广，又称离散拉普拉斯算子。<strong>拉普拉斯算子在图像处理中考虑了当前像素点和相邻像素点（4个）的关系，而拉普拉斯矩阵则考虑了图数据中相邻节点的关系</strong>。对于节点空间$V$和节点属性值空间$X$，存在映射$f:V\rightarrow X$，则其拉普拉斯矩阵运算为对应属性值的散度向量，每个维度代表对应节点的散度值。</p>
<script type="math/tex; mode=display">
L\boldsymbol{x}=(D-A)\boldsymbol{x}=
\begin{bmatrix}
    ...                           \\
    \sum_{v_j\in N(v_i)}(x_i-x_j) \\
    ...
\end{bmatrix}
\tag{4-11}</script><p>上式中每个元素其实就是该节点与相邻节点的差的总和。<br>进一步我们可以得到图的总变差$TV(x)$，即散度与值的乘积再求和。</p>
<script type="math/tex; mode=display">
TV(x)=x^TLx=\sum_{v_i}\sum_{v_j\in N(v_i)}x_i(x_i-x_j)=\sum_{e_{ij}\in E}(x_i-x_j)^2
\tag{4-12}</script><p>上式转化很好理解，对应任意$x_i(x_i-x_j)$项，总对应存在$x_j(x_j-x_i)$项，提取合并同类项后即为$(x_i-x_j)^2$。$E$是关联边集。总变差刻画了图信号整体平滑度。总变差是散度向量的加权求和。</p>
<h2 id="4-3-图傅里叶变换的定义"><a href="#4-3-图傅里叶变换的定义" class="headerlink" title="4.3 图傅里叶变换的定义"></a>4.3 图傅里叶变换的定义</h2><p>众所周知，傅里叶变换将空域信号转换到频域信号，在信号的去噪、压缩和重构等任务上发挥着巨大的作用。在这里，类似的有图的傅里叶变换。</p>
<p>显然，一个拉普拉斯矩阵是一个实对称矩阵，可以被正交对角化，即</p>
<script type="math/tex; mode=display">
L=V\Lambda V^T=[v_1,v_2,\cdots,v_n]
\begin{bmatrix}
    \lambda_1               \\
    & \lambda_2             \\
    && \ddots               \\
    &&& \lambda_n
\end{bmatrix}
\begin{bmatrix}
    v_1^T                   \\
    v_2^T                   \\
    \vdots                  \\
    v_n^T
\end{bmatrix}
\tag{4-13}</script><p>其中V为正交矩阵，$\Lambda$为对角矩阵，而V的列向量就是特征向量，它们是线性无关的单位向量，而$\lambda$则为对应的特征值。我们对特征值进行升序排序。此外根据总变差$\sum_{v_j\in N(v_i)}(x_i-x_j) \ge 0$，可知拉普拉斯矩阵为<strong>半正定矩阵</strong>，特征值均不小于0（半正定矩阵的充要条件）。且可以证明对于拉普拉斯矩阵，最小特征值$\lambda_1 = 0$。</p>
<p>我们将$V=(v_1,v_2,\cdots,v_n)$的<strong>每一个特征向量称为傅里叶基</strong>。对于图G上的任意信号$\boldsymbol{x}=[x_1,x_2,\cdots,x_n]^T$，其元素级别的图傅里叶变换（Graph Fourier Transform, GFT）为：</p>
<script type="math/tex; mode=display">
\tilde{x_k}=\boldsymbol{<v_k,x>}=\boldsymbol{v_k^Tx}=\sum_{i=1}^n v_{ki}x_i
\tag{4-14}</script><p>即与特征向量$v_k$的内积。其矩阵级别为</p>
<script type="math/tex; mode=display">
\boldsymbol{\tilde{x} = V^T x} \tag{4-15}</script><p>由于$V$为正交矩阵，故有如下<strong>逆图傅里叶变换</strong>：</p>
<script type="math/tex; mode=display">
\boldsymbol{
    V\tilde{x}=VV^Tx=Ex=x
}
\tag{4-16}</script><script type="math/tex; mode=display">
\boldsymbol{x}=
    \tilde{x_1}\boldsymbol{v_1} +
    \tilde{x_2}\boldsymbol{v_2} +
    \cdots +
    \tilde{x_n}\boldsymbol{v_n}

\tag{4-17}</script><p>因此，<strong>拉普拉斯矩阵的特征向量实际上是n维空间的一组标准正交基</strong>。而任意图信号均可以由这组基进行线性组合表示。系数称为傅里叶系数。</p>
<h2 id="4-4-空域到频域的转换"><a href="#4-4-空域到频域的转换" class="headerlink" title="4.4 空域到频域的转换"></a>4.4 空域到频域的转换</h2><p>对于一个图信号$\boldsymbol{x}=[x_1,x_2,\cdots,x_n]^T$，<strong>我们定义其$L_2$范数为整个图信号的能量</strong>。则有</p>
<script type="math/tex; mode=display">
\begin{aligned}
E(\bm{x})
       & =\bm{\left\|x\right\|} \\
       & =\bm{(V\tilde{x})^T (V\tilde{x})} \\
       & =\bm{\tilde{x}^T V^T V \tilde{x}} \\
       & =\bm{\tilde{x}^T \tilde{x}}       \\
       & =\sum_{i=1}^n \tilde{x}_i^2
\end{aligned}
\tag{4-18}</script><p>在这里，$x$的$L_2$范数是空域信号（图上的），而傅里叶系数$\tilde{x}$的$L_2$范数是频域信号。为什么傅里叶系数代表了频域信号，且看下文。</p>
<p>如前可知，总变差$TV(x)$描述了一个图的信号平滑度，不同图信号的总变差往往是不一样的（有点类似指纹h）。我们可以通过简单推导知道总变差与特征值之间的关系：</p>
<script type="math/tex; mode=display">
\begin{aligned}
    TV(\bm{x})
    & =\bm{x^TLx}               \\
    & =\bm{x^T V \Lambda V^T x} \\
    & =\bm{(V\tilde{x})^T V \Lambda V^T (V\tilde{x})} \\
    & =\bm{\tilde{x}^T (V^TV) \Lambda (V^TV) \tilde{x}} \\
    & =\bm{\tilde{x}^T \Lambda \tilde{x}} \\
    & =\sum_{i=1}^n \lambda_i \tilde{x}_i^2
\end{aligned}
\tag{4-19}</script><p>在进行分析时，需要先将图信号$\bm{x}$进行归一化处理：</p>
<script type="math/tex; mode=display">
\bm{x_{norm}}=\bm{\frac{x}{\sqrt{\left\|x\right\|}}} \tag{4-20}</script><p>此时由(4-18)，有$\sum_{i=1}^n \tilde{x}_i^2=1$。前面说过我们对特征值进行了升序处理，因此易得</p>
<script type="math/tex; mode=display">
TV(\bm{x})=\sum_{i=1}^n \lambda_i \tilde{x}_i^2
    \ge \lambda_1 \sum_{i=1}^n \tilde{x}_i^2
    = \lambda_1
    \tag{4-21}</script><p>当$\bm{x}=\sum_{i=1}^n \tilde{x}_i \bm{v_i}=\pm\bm{v_1}$，即$\tilde{x}_1=\pm1$时，等号成立。</p>
<p>将傅里叶系数和特征值构建一个离散谱图，<strong>那么特征值就是一种等效频率</strong>，傅里叶系数则是频率信号强度。低频率时总变差小，图信号变化越缓慢。从而进行从空域到频域的转换。如图看到$G_1$的信号平滑，频率信号主要分别在低频段。<br><img src="GFT.jpg" alt="空域与频域的信号"></p>
<h2 id="参考文献-1"><a href="#参考文献-1" class="headerlink" title="参考文献"></a>参考文献</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/85287578">拉普拉斯矩阵和拉普拉斯算子</a> </p>
<h1 id="五、图滤波器"><a href="#五、图滤波器" class="headerlink" title="五、图滤波器"></a>五、图滤波器</h1><p>图率波器从空域角度讲，就是对每个顶点信号的调节。从空频域角度讲，就是对频率强度的处理，或突出高频，或突出低频，或进行调和。</p>
<p>定义图滤波器$H \in R^{N\times N}$，映射$H:R^N \rightarrow R^N$：</p>
<script type="math/tex; mode=display">
H=V \begin{bmatrix}
    h(\lambda_1)     \\
    & h(\lambda_2)   \\
    && \ddots        \\
    &&& h(\lambda_n)
\end{bmatrix}
V^T
\tag{5-1}</script><p>则有如下滤波运算</p>
<script type="math/tex; mode=display">
\bm{y}=\bm{Hx}=\sum_{k=1}^n h(\lambda_k) \tilde{x}_k \bm{v_k} \tag{5-2}</script><p>即对顶点信号强度进行了调节，调节系数和特征值有关，即频率响应函数$h(\lambda)$。事实上，我们可以看到图滤波矩阵$H$与拉普拉斯矩阵$L$唯一的区别在于中间的对角矩阵。因此容易知道，它们具有相同的矩阵结构特征：（1）它们都是实对称矩阵；（2）即当且仅当顶点i和顶点j相关联或相同时，对应值为0（决定相乘为0的数是0，而对角值不为0）。因此，<strong>拉普拉斯矩阵本身也是一个特殊的图滤波矩阵（图位移算子）</strong>。图滤波有下列性质</p>
<ul>
<li>线性：$H(x+y)=Hx+Hy$。这其实就是矩阵相乘的分配律</li>
<li>顺序无关：$H_1(H_2x)=H_2(H_1x)$，容易证明，$H_1$和$H_2$是可交换矩阵。</li>
<li>$h(\lambda)\ne 0$，则滤波可逆。显然，分母不能为0。</li>
</ul>
<p>不同的频率响应函数$h(\lambda)$对应了不同类型的图滤波器。</p>
<ul>
<li>低通滤波：顾名思义是低频率能通过，高频率为归零。关注平滑部分。</li>
<li>高通滤波：反之，关注突变部分。</li>
<li>带通滤波：只通过感兴趣的频段。</li>
</ul>
<p>如前，滤波器和拉普拉斯矩阵存在的联系，因此我们可以两者看做一种映射$f:L \rightarrow H$。通过泰勒展开，保留前$k$项，有：</p>
<script type="math/tex; mode=display">
H=f(L)=h_0L^0 + h_1L^1 + h_2L^2 + \cdots + h_kL^k = \sum_{i=1}^k h_kL^k \tag{5-3}</script><p>通过调整系数向量$\bm{h} = (h_1,h_2,\cdots, h_k)$，拟合出任意图滤波器。</p>
<h2 id="5-1-空域角度"><a href="#5-1-空域角度" class="headerlink" title="5.1 空域角度"></a>5.1 空域角度</h2><p>将(5-3)代入5-2，有</p>
<script type="math/tex; mode=display">
\bm{y}=\bm{Hx}=h_0L^0\bm{x} + h_1L^1\bm{x} + h_2L^2\bm{x} + \cdots + h_kL^k\bm{x} \tag{5-4}</script><p>不妨假设第k项</p>
<script type="math/tex; mode=display">
\bm{x^{(k)}}=L^k\bm{x} \\
有\bm{x^{(k)}}=L\bm{x^{(k-1)}} \tag{5-5}</script><p>则有</p>
<script type="math/tex; mode=display">
\bm{y}=\sum_{i=1}^k h_k \bm{x^{(k)}} \tag{5-6}</script><p>显然从$\bm{x^{(k-1)}}$到$\bm{x^{(k)}}$需要用拉普拉斯矩阵进行一次变换，涉及所有一阶邻居（根据公式4-11）。而从$\bm{x^{(0)}}$到$\bm{x^{(k)}}$，需要经历k次变换，故最终结果涉及k阶子图所有节点。因此图滤波在空域上有如下特性：</p>
<ul>
<li>局部性，每个节点的输出至于其k阶子图相关，类似卷积运算卷积核。</li>
<li>可迭代，基于公式5-5，显然可以通过迭代运算快速完成。</li>
</ul>
<p>上述推导中利用了拉普拉斯矩阵$L$，但实际上$L$可以用其他图位移算子代替，任何图位移算子都可以对$L$进行泰勒展开，由泰勒展开的线性可逆性，同样可以用其他图位移算子。即</p>
<script type="math/tex; mode=display">
H_2=f(H_1)=\sum_{i=1}^k h_kH_1^k \tag{5-7}</script><p>由于空域计算是迭代运算，我们一般也把$H_1$称为图滤波器（虽然前面一开始是用$H_2$）推导的。</p>
<h2 id="5-2-频域角度"><a href="#5-2-频域角度" class="headerlink" title="5.2 频域角度"></a>5.2 频域角度</h2><script type="math/tex; mode=display">
\begin{aligned}
    H &= \sum_{i=1}^k h_kL^k  \\
      &= \sum_{i=1}^k h_k(V \Lambda V^T)^k \\
      &= \sum_{i=1}^k h_k(V \Lambda V^T V \Lambda V^T)(V \Lambda V^T)^{k-2} \\
      &= \sum_{i=1}^k h_k(V \Lambda E \Lambda V^T)(V \Lambda V^T)^{k-2} \\
      &= \sum_{i=1}^k h_k(V \Lambda^2 V^T)(V \Lambda V^T)^{k-2} \\
      &= \sum_{i=1}^k h_kV \Lambda^k V^T \\
      &= V (\sum_{i=1}^k h_k \Lambda^k) V^T \\
      &=V\Lambda_h V^T \tag{5-8}
\end{aligned}</script><p>其中</p>
<script type="math/tex; mode=display">
\Lambda_h=\sum_{i=1}^k h_k \Lambda^k=\begin{bmatrix}
    \sum_{i=1}^k h_k \lambda_1^k \\
    & \sum_{i=1}^k h_k \lambda_2^k \\
    && \ddots \\
    &&& \sum_{i=1}^k h_k \lambda_n^k \tag{5-9}
\end{bmatrix}</script><p>即响应函数$h(\lambda)=\sum_{i=1}^k h_k \lambda^k$。那么频域视角下，有</p>
<script type="math/tex; mode=display">
\bm{y}=\bm{Hx}=V\Lambda_h V^T\bm{x} \\
即 V^T \bm{y} = \Lambda_h V^T\bm{x} \\
即 \tilde{\bm{y}} = \Lambda_h \tilde{\bm{x}} \tag{5-10}</script><p>即滤波过程分为三步</p>
<ul>
<li>空域转频域</li>
<li>频率强度调节：利用$\Lambda_h$</li>
<li>频域转空域</li>
</ul>
<p>滤波过程中第二步的$\Lambda_h$可以用范德蒙矩阵$\Psi$和系数向量$\bm{h}$表达。</p>
<script type="math/tex; mode=display">
\Lambda_h=\sum_{i=1}^k h_k \Lambda^k=\text{diag}(\Psi \bm{h}) \tag{5-11}</script><p>diag运算将向量转为对角矩阵。其中范德蒙矩阵由特征值定义：</p>
<script type="math/tex; mode=display">
\Psi = \begin{bmatrix}
    1       & \lambda_1 & \cdots & \lambda_1^k \\
    1       & \lambda_2 & \cdots & \lambda_2^k \\
    \vdots  & \vdots    & \ddots & \vdots      \\
    1       & \lambda_n & \cdots & \lambda_n^k \\
\end{bmatrix}
\tag{5-12}</script><p>通过逆运算也可以求出系数向量。</p>
<script type="math/tex; mode=display">
\bm{h}=\Psi^{-1} \text{diag}^{-1}(\Lambda_h) \tag{5-13}</script><p>相比空域视角，频域视角对滤波过程更加清晰，且可以显式指导滤波器设计。但是特征分解更加耗时，时间复杂度较高，$O(N^3)$。</p>
<h1 id="六、图卷积神经网络"><a href="#六、图卷积神经网络" class="headerlink" title="六、图卷积神经网络"></a>六、图卷积神经网络</h1><h2 id="6-1-图卷积"><a href="#6-1-图卷积" class="headerlink" title="6.1 图卷积"></a>6.1 图卷积</h2><h3 id="卷积定理"><a href="#卷积定理" class="headerlink" title="卷积定理"></a>卷积定理</h3><p>函数卷积的傅里叶变换是函数傅里叶变换的乘积。</p>
<script type="math/tex; mode=display">FT(\bm{f*g})=FT(\bm{f})FT(\bm{g})</script><h3 id="图卷积的基础定义"><a href="#图卷积的基础定义" class="headerlink" title="图卷积的基础定义"></a>图卷积的基础定义</h3><p>在CNN中，卷积运算是一个权重矩阵（卷积核）与输入的信号的哈达玛积的和。即$\text{sum}(\bm{w}\odot \bm{x}) + b$。类似的，图卷积（GC）是一个输入信号$\bm{x^N}$和“权重”信号$\bm{w^N}$，转到频域后求哈达玛积再转回空域。</p>
<script type="math/tex; mode=display">
GC(\bm{w, x})=IGFT(GFT(\bm{w}) \odot GFT(\bm{x})) \tag{6-1}</script><p>GFT是图傅里叶变换，从空域转频域；IGFT是逆图傅里叶变换，从频域转空域。上式代入具体变换公式，有</p>
<script type="math/tex; mode=display">
\begin{aligned}
    GC(\bm{w, x}) &=V((V^T \bm{w}) \odot (V^T \bm{x}))  \\
                  &=V(\bm{\tilde{x}} \odot (V^T \bm{x})) \\
                  &=V\text{diag}(\bm{\tilde{x}}) V^T \bm{x} \\
                  &=H_w\bm{x}
\end{aligned}
\tag{6-2}</script><p>卷积也称滤波，从上述推导，我们可以看到，<strong>图卷积的本质就是图滤波</strong>。事实上，公式6-2是更为常用的图卷积计算形式。</p>
<h3 id="图卷积的通道"><a href="#图卷积的通道" class="headerlink" title="图卷积的通道"></a>图卷积的通道</h3><p>在图像处理中，图片的输入往往是NCHW形式，其中C指的是图像的通道数，每个通道是图像的不同色域特征。类似的，一个图$G=(V,E)$中的每个节点，也会存在多个通道，就如一个人会有身高、体重等多项指标。因此，<strong>输入信号从向量转为矩阵，即$x^N \rightarrow X^{N \times D}$</strong>，其中D为通道数。此时，图卷积计算变为$H:X^{N \times D} \rightarrow Y^{N \times D}$，即</p>
<script type="math/tex; mode=display">
Y=H_wX \tag{6-3}</script><h2 id="6-2-以频率响应矩阵为学习参数"><a href="#6-2-以频率响应矩阵为学习参数" class="headerlink" title="6.2 以频率响应矩阵为学习参数"></a>6.2 以频率响应矩阵为学习参数</h2><p>以频率响应矩阵$\text{diag}(\tilde{\bm{w}})$为学习参数，同时类比卷积神经网络引入激活函数，有以下基本图卷积：</p>
<script type="math/tex; mode=display">
\begin{aligned}
    Y &=\sigma(V\text{diag}(\tilde{\bm{w}})V^TX) \\
      &=\sigma(V\begin{bmatrix}
          \tilde{w}_1 \\
          & \tilde{w}_2 \\
          && \ddots  \\
          &&& \tilde{w}_n
      \end{bmatrix}
      V^TX)
\end{aligned}
\tag{6-4}</script><p>上式中$\sigma$是激活函数，而频率响应矩阵的值就是待学习参数。<strong>实际上，真实训练中只优化前k个低频参数。因为高频段信息较少，且节点数量太多不可能参数一样多。</strong></p>
<h2 id="6-3-以多项式系数为学习参数"><a href="#6-3-以多项式系数为学习参数" class="headerlink" title="6.3 以多项式系数为学习参数"></a>6.3 以多项式系数为学习参数</h2><p>前面介绍了图滤波器的泰勒展开，里面有多项式系数向量$\bm{h}=[h_1,h_2,\cdots,h_k]^T$，由公式5-10和5-11，同理推广至多通道并引入激活函数，有</p>
<script type="math/tex; mode=display">
Y=\sigma(V\Lambda_hV^TX)=\sigma(V\text{diag}(\Psi \bm{h})V^TX) \tag{6-5}</script><p>前面说过。这里的参数量有阶数k决定。阶数越高，所涉及的邻近节点越多（k阶邻居）。一般$k &lt;&lt; N$，防止过拟合。</p>
<h2 id="6-4-固定图滤波器——GCN"><a href="#6-4-固定图滤波器——GCN" class="headerlink" title="6.4 固定图滤波器——GCN"></a>6.4 固定图滤波器——GCN</h2><p>多项式法虽然大大减少了参数量，但是需要特征分解求特征值，计算复杂度高。为此，人们又做了简化。</p>
<p>首先，固定阶数k=1，即</p>
<script type="math/tex; mode=display">
Y=\sigma(h_0X + h_1LX) \tag{6-6}</script><p>再固定系数均为1，即</p>
<script type="math/tex; mode=display">
Y=\sigma((E+L)X) = \sigma(\tilde{L}X) \tag{6-7}</script><p>对$\tilde{L}$进行归一化为$\tilde{L}_{sym}$，即</p>
<script type="math/tex; mode=display">
\begin{aligned}
    \tilde{A}       &= A + E \\
    \tilde{D}_{ii}  &=\sum_j\tilde{A}_{ij} \\
    \tilde{L}_{sym} &=\tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2}
\end{aligned}
\tag{6-8}</script><p>此时固定了滤波器，滤波器相当于一个固定的特征提取算子。那么此时训练学习什么？为此人们引入了权重矩阵$W^{d \times d’}$对输入$X^{N \times d}$做仿射变换（即$Ax+b$），训练的参数就是权重矩阵。权重矩阵与卷积核类似，将一个节点的特征（即前面的通道）进行整合，即$d \rightarrow d’$。</p>
<script type="math/tex; mode=display">
Y=\sigma(\tilde{L}_{sym}XW) \tag{6-9}</script><p>上式就是图卷积层（GCN layer）的计算公式,由此而来的就是就是图卷积神经网络。</p>
<h2 id="七、基于空域的图神经网络"><a href="#七、基于空域的图神经网络" class="headerlink" title="七、基于空域的图神经网络"></a>七、基于空域的图神经网络</h2><h3 id="7-1-距离空间"><a href="#7-1-距离空间" class="headerlink" title="7.1 距离空间"></a>7.1 距离空间</h3><p>对于一个非空集合$X$，对于$\forall x, y \in X$，$d(x, y)$表示两个元素之间的距离，其满足三大公理：</p>
<ul>
<li>非负性：$d(x, y) \ge 0$，当且仅当$x = y，d(x, y)=0$</li>
<li>对称性：$d(x, y) = d(y, x)$</li>
<li>三角不等性：$d(x, y) \le d(x, z) + d(z, y)$</li>
</ul>
<p>特称非空集合$X$为距离空间，记为(X, d)。也称度量空间。</p>
<h3 id="7-2-压缩映射"><a href="#7-2-压缩映射" class="headerlink" title="7.2 压缩映射"></a>7.2 压缩映射</h3><p>$T: X \rightarrow X$为距离空间$(X, d)$的一个映射，满足$\exist q \in [0,1), d(T(x), T(y)) \le q d(x, y)$，称T为压缩映射，q为压缩常数。即映射后的距离小于映射前。</p>
<h3 id="7-3-巴拿赫不动点定理"><a href="#7-3-巴拿赫不动点定理" class="headerlink" title="7.3 巴拿赫不动点定理"></a>7.3 巴拿赫不动点定理</h3><p>该定理说的是，在一个完备度量空间中，一个压缩映射$T$，有且仅有一个$x^<em>$，满足$T(x^</em>)=x^*$，即</p>
<h2 id="参考文献-2"><a href="#参考文献-2" class="headerlink" title="参考文献"></a>参考文献</h2><ol>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/luoxuexiong/article/details/102315643">图卷积神经网络详解</a></li>
<li><a target="_blank" rel="noopener" href="http://tkipf.github.io/graph-convolutional-networks/">图卷积作者博客</a></li>
</ol>
</div><div class="post-copyright"><div class="post-copyright-author"><span class="post-copyright-meta">本文作者: </span><span class="post-copyright-info"><a href="mailto:undefined">GCS-ZHN</a></span></div><div class="post-copyright-type"><span class="post-copyright-meta">本文链接: </span><span class="post-copyright-info"><a href="https://gcszhn.top/2022/03/10/GNN/">https://gcszhn.top/2022/03/10/GNN/</a></span></div><div class="post-copyright-notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://gcszhn.top">潇洒记忆</a>！</span></div></div></article><div id="pagination"><div class="prev-post pull-left"><span class="line line-top"></span><span class="line line-right"></span><span class="line line-bottom"></span><span class="line line-left"></span><a href="/2022/03/19/WSL2-and-Docker/"><i class="fas fa-angle-left">&nbsp;</i><span>WSL2和Docker for Windows</span></a></div><div class="next-post pull-right"><span class="line line-top"></span><span class="line line-right"></span><span class="line line-bottom"></span><span class="line line-left"></span><a href="/2022/03/10/git/"><span>Git学习笔记</span><span>&nbsp;</span><i class="fas fa-angle-right"></i></a></div></div><!--div!= paginator()--></div></div><div class="button-hover" id="return-top"><i class="fas fa-arrow-up" aria-hidden="true"></i></div><footer><div id="footer"><div class="button-hover" id="side-button"><i class="fas fa-arrow-right"></i></div><div class="right-content"><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fas fa-file-o"></i></span><span id="busuanzi_value_page_pv"></span><span></span></div><div class="copyright">&copy;2017 ～ 2022 By GCS-ZHN</div><a href="https://beian.miit.gov.cn/" target="_blank">&nbsp; 浙ICP备2022014039号</a></div></div></footer></div><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/jquery-3.3.1.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/velocity.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/jquery.mCustomScrollbar.concat.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/fan.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/canvas_bg.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/utils.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/scroll.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/sidebar.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/copy.js"></script><!--script(src=url)--></body></html>