<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="BindCraft算法解析和测试"><meta name="keywords" content="antibody design,ai,protein design,alphafold"><meta name="author" content="GCS-ZHN,undefined"><meta name="copyright" content="GCS-ZHN"><title>BindCraft算法解析和测试【潇洒记忆】</title><link rel="stylesheet" href="/css/fan.css"><link rel="stylesheet" href="/css/thirdparty/jquery.mCustomScrollbar.min.css"><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"><link rel="icon" href="/favicon.ico"><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- script(src=url_for("/js/mathjax/mathjax.js"))--><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
});
</script><script>var isPassword = '' || false;
if (isPassword) {
    if (prompt('请输入文章密码') !== '') {
        alert('密码错误！');
        history.back();
    }
}</script><script>window.GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  gitment: {},
  valine: {},
}</script><meta name="generator" content="Hexo 6.0.0"></head><body><canvas id="universe"></canvas><!--#body--><div id="sidebar"><div class="toggle-sidebar-info button-hover"><span data-toggle="文章目录">站点概览</span></div><div class="sidebar-toc"><div class="sidebar-toc-title">目录</div><div class="sidebar-toc-progress"><span class="progress-notice">您已阅读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc-progress-bar"></div></div><div class="sidebar-toc-content" id="sidebar-toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E9%83%A8%E7%BD%B2"><span class="toc-number">1.</span> <span class="toc-text">算法部署</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E4%BB%A3%E7%A0%81%E7%9A%84%E7%AE%97%E6%B3%95%E8%A7%A3%E6%9E%90"><span class="toc-number">2.</span> <span class="toc-text">基于代码的算法解析</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%BA%8F%E5%88%97logits%E7%94%9F%E6%88%90"><span class="toc-number">2.1.</span> <span class="toc-text">初始序列logits生成</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BA%8F%E5%88%97logits%E7%9A%84SoftMax%E4%BC%98%E5%8C%96"><span class="toc-number">2.2.</span> <span class="toc-text">序列logits的SoftMax优化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BA%8F%E5%88%97Onehot%E4%BC%98%E5%8C%96%EF%BC%88ArgMax%EF%BC%89"><span class="toc-number">2.3.</span> <span class="toc-text">序列Onehot优化（ArgMax）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ProteinMPNN%E9%9D%9E%E6%8E%A5%E8%A7%A6%E5%BA%8F%E5%88%97%E4%BC%98%E5%8C%96"><span class="toc-number">2.4.</span> <span class="toc-text">ProteinMPNN非接触序列优化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%BE%E8%AE%A1%E7%BB%93%E6%9E%9C%E7%9A%84%E6%8C%87%E6%A0%87%E7%AD%9B%E9%80%89"><span class="toc-number">2.5.</span> <span class="toc-text">设计结果的指标筛选</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%94%9F%E6%88%90%E6%B5%8B%E8%AF%95"><span class="toc-number">3.</span> <span class="toc-text">生成测试</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#PDL1"><span class="toc-number">3.1.</span> <span class="toc-text">PDL1</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info-avatar"><img class="author-info-avatar-img" src="/avatar.png"></div><div class="author-info-name">GCS-ZHN</div><div class="author-info-description"></div><div class="links-buttons"><a class="links-button button-hover" href="https://github.com/GCS-ZHN" target="_blank">GitHub<i class="icon-dot bg-color5"></i></a><a class="links-button button-hover" href="mailto:zhang.h.n@foxmail.com" target="_blank">E-Mail<i class="icon-dot bg-color0"></i></a></div><div class="author-info-articles"><a class="author-info-articles-archives article-meta" href="/archives"><span class="pull-top">日志</span><span class="pull-bottom">38</span></a><a class="author-info-articles-tags article-meta" href="/tags"><span class="pull-top">标签</span><span class="pull-bottom">67</span></a><a class="author-info-articles-categories article-meta" href="/categories"><span class="pull-top">分类</span><span class="pull-bottom">22</span></a></div></div></div><div id="main-container"><header><div id="menu-outer"><i class="menu-list-icon fas fa-bars"></i><nav id="menu-inner"><a class="menu-item" href="/">首页</a><a class="menu-item" href="/archives">归档</a></nav><div class="right-info"><a class="title-name" href="/">潇洒记忆</a><span id="now-time"></span></div></div></header><div id="content-outer"><div id="content-inner"><article id="post"><div class="post-header"><div class="title">BindCraft算法解析和测试</div><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 发表于 2024-10-23 | 更新于 2025-02-25</time><!--time.button-hover.post-date #[i.fas.fa-calendar-alt.article-icon(aria-hidden="true")] #[=__('post.modified')] #[=date(page['updated'], config.date_format)]--><div class="button-hover categories"><i class="fa fa-inbox article-icon" aria-hidden="true"></i><a class="link-a" href="/categories/AI4S/">AI4S</a><i class="fa fa-angle-right" style="margin: 0 8px;"></i><i class="fa fa-inbox article-icon" aria-hidden="true"></i><a class="link-a" href="/categories/AI4S/%E8%9B%8B%E7%99%BD%E8%AE%BE%E8%AE%A1/">蛋白设计</a></div><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="/tags/antibody-design/">antibody design</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="/tags/ai/">ai</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="/tags/protein-design/">protein design</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="/tags/alphafold/">alphafold</a></div></div></div><div class="main-content"><p>这篇文章pipeline的核心工具是colabdesign（Sergey Ovchinnikov团队研发，故而Sergey作为BindCraft的共同通讯），利用colabdesign提供的afdesign模块和proteinMPNN模块，进行binder design。</p>
<p>原文链接：<a target="_blank" rel="noopener" href="https://www.biorxiv.org/content/10.1101/2024.09.30.615802">https://www.biorxiv.org/content/10.1101/2024.09.30.615802</a></p>
<p>代码链接：<a target="_blank" rel="noopener" href="https://github.com/martinpacesa/BindCraft">https://github.com/martinpacesa/BindCraft</a></p>
<h1 id="算法部署"><a href="#算法部署" class="headerlink" title="算法部署"></a>算法部署</h1><span id="more"></span>
<p>算法是基于colabdesign开发的，需要部署colabdesign环境，下载colabfold提供的alphafold multimer权重（可以各个weight都下载，反复用）。</p>
<h1 id="基于代码的算法解析"><a href="#基于代码的算法解析" class="headerlink" title="基于代码的算法解析"></a>基于代码的算法解析</h1><p>算法主过程采用四阶段优化，同时可以额外用proteinMPNN（soluble版）进行binder性质改善。然后生成的结果进行综合的打分评估。算法的核心在于下列自定义hallucination loss组合，其中给contact loss（intra+inter）最高的权重，从而诱导生成具有一定靶向能力和二级结构的binder。</p>
<ul>
<li>Binder confidence pLDDT (weight 0.1)</li>
<li>Interface confidence i_pTM (weight 0.05)</li>
<li>Normalized predicted alignment error (pAE) within the binder (weight 0.4)</li>
<li>Normalized predicted alignment error (pAE) between binder and target (weight 0.1) </li>
<li>Residue contact loss within binder (weight 1.0) </li>
<li>Residue contact loss between the target and binder - if hotspots are specified, the rest of the target is masked from this loss (weight 1.0) </li>
<li>Radius of gyration of binder (weight 0.3)</li>
<li>“Helicity loss” - penalize or promote backbone contacts every in a 3 residue offset to promote the hallucination of helical or non-helical designs (weight -0.3)</li>
</ul>
<p>与训练时BP过程相似，不同的是，训练过程利用权重的梯度信息更新模型权重，而这里数据的梯度信息更新序列embedding logits。同时多批数据训练类似，这里用上五个权重以减少设计偏差。因此序列初始化也同样会影响优化结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">mk_af_model</span>(design_model, _af_inputs, _af_loss, _af_prep, _af_design, _af_utils):</span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">               protocol=<span class="string">&quot;fixbb&quot;</span>, </span></span><br><span class="line"><span class="params">               use_multimer=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">               use_templates=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">               debug=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">               data_dir=<span class="string">&quot;.&quot;</span>, </span></span><br><span class="line"><span class="params">               **kwargs</span>):  </span><br></pre></td></tr></table></figure>
<p>colabdesign的模型对象是通过多继承的实现：</p>
<ul>
<li>design_model实现了一个通用的处理逻辑（不止用在afdesign），</li>
<li>_af_design实现了基于alphafold的design过程，</li>
<li>_af_inputs实现对输入结构的处理，</li>
<li>_af_prep实现对binder序列的初始化，</li>
<li>_af_loss接入了loss函数实现</li>
</ul>
<p>本身这个实现架构还是可以的，但colabdesign的代码问题在于，上述每个父类模块之间会存在相互的方法调用，而这些父类本身没有继承关系。这就导致在代码阅读时经常找不到某个方法。同时里面有大量函数式编程的痕迹（如闭包），从而使得代码阅读和修改存在较大挑战。</p>
<h2 id="初始序列logits生成"><a href="#初始序列logits生成" class="headerlink" title="初始序列logits生成"></a>初始序列logits生成</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Stage 1: Test Logits&quot;</span>)</span><br><span class="line">af_model.design_logits(iters=<span class="number">50</span>, e_soft=<span class="number">0.9</span>, models=design_models, num_models=<span class="number">1</span>, sample_models=advanced_settings[<span class="string">&quot;sample_models&quot;</span>], save_best=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>这一阶段是初始序列logits迭代优化的一个阶段，默认设置迭代50次。其核心是利用反向传播算法进行序列logits的梯度下降优化，这里的grad和params的shape为（1，L，20）。</p>
<p>论文中对此部分的序列logits的公式描述如下，即分为未曾归一化的logits和softmax归一化后的logit的一个线性组合:</p>
<script type="math/tex; mode=display">
logits=(1-\lambda)\times logits + \lambda\times softmax(\frac{logits}{temperature})</script><script type="math/tex; mode=display">
\lambda=\frac{(step+1)}{iterations}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># colabdesign.shared.model.soft_seq</span></span><br><span class="line"><span class="comment"># 相关代码部分内容</span></span><br><span class="line">seq[<span class="string">&quot;soft&quot;</span>] = jax.nn.softmax(seq[<span class="string">&quot;logits&quot;</span>] / opt[<span class="string">&quot;temp&quot;</span>])</span><br><span class="line">seq[<span class="string">&quot;pseudo&quot;</span>] = opt[<span class="string">&quot;soft&quot;</span>] * seq[<span class="string">&quot;soft&quot;</span>] + (<span class="number">1</span>-opt[<span class="string">&quot;soft&quot;</span>]) * seq[<span class="string">&quot;input&quot;</span>]</span><br></pre></td></tr></table></figure>
<p>也就是说随着迭代过程，logits会更加偏向softmax的部分。这样设计的一种意义是可以让前期快速梯度下降，然后平滑的将logits过度到softmax之后的概率分布（softmax之后的logits其实就是一个PSSM矩阵）。不过实际测试代码可以看到系数lambda的值应当成上e_soft=0.9，这与论文有一点不同。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lr = self.opt[<span class="string">&quot;learning_rate&quot;</span>] * lr_scale</span><br><span class="line">self._params = jax.tree_map(<span class="keyword">lambda</span> x,g:x-lr*g, self._params, self.aux[<span class="string">&quot;grad&quot;</span>])</span><br></pre></td></tr></table></figure>
<p>如果初始迭代后得到的最佳plddt&gt;0.65（后续也是这样），才会继续后续优化，否则会重新初始化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> advanced_settings[<span class="string">&quot;optimise_beta&quot;</span>]:</span><br><span class="line">    <span class="comment"># temporarily dump model to assess secondary structure</span></span><br><span class="line">    af_model.save_pdb(model_pdb_path)</span><br><span class="line">    _, beta, *_ = calc_ss_percentage(model_pdb_path, advanced_settings, <span class="string">&#x27;B&#x27;</span>)</span><br><span class="line">    os.remove(model_pdb_path)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># if beta sheeted trajectory is detected then choose to optimise</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">float</span>(beta) &gt; <span class="number">15</span>:</span><br><span class="line">        advanced_settings[<span class="string">&quot;soft_iterations&quot;</span>] = advanced_settings[<span class="string">&quot;soft_iterations&quot;</span>] + advanced_settings[<span class="string">&quot;optimise_beta_extra_soft&quot;</span>]</span><br><span class="line">        advanced_settings[<span class="string">&quot;temporary_iterations&quot;</span>] = advanced_settings[<span class="string">&quot;temporary_iterations&quot;</span>] + advanced_settings[<span class="string">&quot;optimise_beta_extra_temp&quot;</span>]</span><br><span class="line">        af_model.set_opt(num_recycles=advanced_settings[<span class="string">&quot;optimise_beta_recycles_design&quot;</span>])</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Beta sheeted trajectory detected, optimising settings&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>正如论文中补充说明的，算法在检测到trajectory存在明显的beta-sheet时，会自动增加recycle的数量以盼提高预测准确性。这可能是因为beta-sheet结构更加稳定规律，因此需要更高的准确性。但其实也可以同时考虑helix。</p>
<p>由于是alphafold进行结构预测，因此在输入预测模型前会根据当前分布得到一个序列氨基酸信息，具体代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># colabdesign.af.model.mk_af_model:_get_model:_model line 167</span></span><br><span class="line">update_aatype(seq[<span class="string">&#x27;pseudo&#x27;</span>][<span class="number">0</span>].argmax(-<span class="number">1</span>), inputs)</span><br></pre></td></tr></table></figure></p>
<h2 id="序列logits的SoftMax优化"><a href="#序列logits的SoftMax优化" class="headerlink" title="序列logits的SoftMax优化"></a>序列logits的SoftMax优化</h2><p>这一个阶段实际上是从上一阶段平滑过渡下来的。设置lambda=1(代码中soft=1)，使得上述<code>soft_seq</code>方法只考虑softmax之后的logits。softmax优化实际上就是得到一个最为合理的PSSM矩阵。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">af_model.design_soft(advanced_settings[<span class="string">&quot;temporary_iterations&quot;</span>], e_temp=<span class="number">1e-2</span>, models=design_models, num_models=<span class="number">1</span>,</span><br><span class="line">    sample_models=advanced_settings[<span class="string">&quot;sample_models&quot;</span>], ramp_recycles=<span class="literal">False</span>, save_best=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h2 id="序列Onehot优化（ArgMax）"><a href="#序列Onehot优化（ArgMax）" class="headerlink" title="序列Onehot优化（ArgMax）"></a>序列Onehot优化（ArgMax）</h2><p>然后onehot优化是在softmax优化的基础上，对每个位置概率最高（argmax）的氨基酸进行概率微调。这里具体实现同样是在上面的<code>soft_seq</code>方法中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">af_model.design_hard(advanced_settings[<span class="string">&quot;hard_iterations&quot;</span>], temp=<span class="number">1e-2</span>, models=design_models, num_models=<span class="number">1</span>,</span><br><span class="line">    sample_models=advanced_settings[<span class="string">&quot;sample_models&quot;</span>], dropout=<span class="literal">False</span>, ramp_recycles=<span class="literal">False</span>, save_best=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>由于argmax运算是不可微的，因此这里通过选择性裁切softmax logits的梯度信息来实现反向传播更新。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">seq[<span class="string">&quot;pssm&quot;</span>] = jax.nn.softmax(seq[<span class="string">&quot;logits&quot;</span>])</span><br><span class="line">seq[<span class="string">&quot;soft&quot;</span>] = jax.nn.softmax(seq[<span class="string">&quot;logits&quot;</span>] / opt[<span class="string">&quot;temp&quot;</span>])</span><br><span class="line">seq[<span class="string">&quot;hard&quot;</span>] = jax.nn.one_hot(seq[<span class="string">&quot;soft&quot;</span>].argmax(-<span class="number">1</span>), seq[<span class="string">&quot;soft&quot;</span>].shape[-<span class="number">1</span>])</span><br><span class="line">seq[<span class="string">&quot;hard&quot;</span>] = jax.lax.stop_gradient(seq[<span class="string">&quot;hard&quot;</span>] - seq[<span class="string">&quot;soft&quot;</span>]) + seq[<span class="string">&quot;soft&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># create pseudo sequence</span></span><br><span class="line">seq[<span class="string">&quot;pseudo&quot;</span>] = opt[<span class="string">&quot;soft&quot;</span>] * seq[<span class="string">&quot;soft&quot;</span>] + (<span class="number">1</span>-opt[<span class="string">&quot;soft&quot;</span>]) * seq[<span class="string">&quot;input&quot;</span>]</span><br><span class="line">seq[<span class="string">&quot;pseudo&quot;</span>] = opt[<span class="string">&quot;hard&quot;</span>] * seq[<span class="string">&quot;hard&quot;</span>] + (<span class="number">1</span>-opt[<span class="string">&quot;hard&quot;</span>]) * seq[<span class="string">&quot;pseudo&quot;</span>]</span><br></pre></td></tr></table></figure>
<h2 id="ProteinMPNN非接触序列优化"><a href="#ProteinMPNN非接触序列优化" class="headerlink" title="ProteinMPNN非接触序列优化"></a>ProteinMPNN非接触序列优化</h2><p>这一步通过固定接触面的设计残基，重新设计其他部位进行性质改善，筛选保留得分最高的两个。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mpnn_trajectories = mpnn_gen_sequence(trajectory_pdb, binder_chain, trajectory_interface_residues, advanced_settings)</span><br></pre></td></tr></table></figure>
<h2 id="设计结果的指标筛选"><a href="#设计结果的指标筛选" class="headerlink" title="设计结果的指标筛选"></a>设计结果的指标筛选</h2><ul>
<li>AF2 confidence pLDDT score of the predicted complex (&gt; 0.8) </li>
<li>AF2 interface predicted confidence score (i_pTM) (&gt; 0.5) </li>
<li>AF2 interface predicted alignment error (i_pAE) (&gt; 0.35) </li>
<li>Rosetta interface shape complementarity (&gt; 0.55) </li>
<li>Number of unsaturated hydrogen bonds at the interface (&lt; 3) </li>
<li>Hydrophobicity of binder surface (&lt; 35%) </li>
<li>RMSD of binder predicted in bound and unbound form (&lt; 3.5 Å)</li>
</ul>
<h1 id="生成测试"><a href="#生成测试" class="headerlink" title="生成测试"></a>生成测试</h1><h2 id="PDL1"><a href="#PDL1" class="headerlink" title="PDL1"></a>PDL1</h2><video width="100%" height="auto" controls autoplay loop>
  <source type="video/mp4" src="/2024/10/23/BindCraft%E7%AE%97%E6%B3%95%E8%A7%A3%E6%9E%90%E5%92%8C%E6%B5%8B%E8%AF%95/pd1_design.mp4">
  Your browser does not support the video tag.
</video></div><div class="post-copyright"><div class="post-copyright-author"><span class="post-copyright-meta">本文作者: </span><span class="post-copyright-info"><a href="mailto:undefined">GCS-ZHN</a></span></div><div class="post-copyright-type"><span class="post-copyright-meta">本文链接: </span><span class="post-copyright-info"><a href="https://gcszhn.top/2024/10/23/BindCraft%E7%AE%97%E6%B3%95%E8%A7%A3%E6%9E%90%E5%92%8C%E6%B5%8B%E8%AF%95/">https://gcszhn.top/2024/10/23/BindCraft算法解析和测试/</a></span></div><div class="post-copyright-notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://gcszhn.top">潇洒记忆</a>！</span></div></div></article><div id="pagination"><div class="prev-post pull-left"><span class="line line-top"></span><span class="line line-right"></span><span class="line line-bottom"></span><span class="line line-left"></span><a href="/2024/10/24/Chroma%E6%B5%8B%E8%AF%95%E4%B8%8Ebinder%E8%AE%BE%E8%AE%A1%E6%8E%A2%E7%B4%A2/"><i class="fas fa-angle-left">&nbsp;</i><span>Chroma测试与binder设计探索</span></a></div><div class="next-post pull-right"><span class="line line-top"></span><span class="line line-right"></span><span class="line line-bottom"></span><span class="line line-left"></span><a href="/2022/11/10/%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B/"><span>高斯混合模型</span><span>&nbsp;</span><i class="fas fa-angle-right"></i></a></div></div><!--div!= paginator()--></div></div><div class="button-hover" id="return-top"><i class="fas fa-arrow-up" aria-hidden="true"></i></div><footer><div id="footer"><div class="button-hover" id="side-button"><i class="fas fa-arrow-right"></i></div><div class="right-content"><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fas fa-file-o"></i></span><span id="busuanzi_value_page_pv"></span><span></span></div><div class="copyright">&copy;2017 ～ 2025 By GCS-ZHN</div><a href="https://beian.miit.gov.cn/" target="_blank">&nbsp; 浙ICP备2022014039号</a></div></div></footer></div><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/jquery-3.3.1.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/velocity.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/jquery.mCustomScrollbar.concat.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/fan.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/canvas_bg.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/utils.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/scroll.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/sidebar.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/copy.js"></script><!--script(src=url)--></body></html>